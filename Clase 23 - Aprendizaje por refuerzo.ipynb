{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c59b012-3057-415a-9eaf-7f225e12525b",
   "metadata": {},
   "source": [
    "# Aprendizaje por refuerzo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d235d-6993-4205-82b5-10457b767b34",
   "metadata": {},
   "source": [
    "El aprendizaje por refuerzo (en inglÃ©s Reinforcement Learning â€” RL) es un tipo de aprendizaje automÃ¡tico en el que un agente aprende a tomar decisiones a travÃ©s de la interacciÃ³n con un ambiente. El agente recibe recompensas o penalidades con base en sus acciones y aprende a maximizar la recompenas acumulada a lo largo del tiempo.\n",
    "\n",
    "Los componentes clave del RL son:\n",
    "\n",
    "- Agente: aprendiz o tomador de decisiones.\n",
    "- Ambiente: todo con lo que agente puede interactuar.\n",
    "- AcciÃ³n: las elecciones que el agente puede hacer.\n",
    "- Estado: la situaciÃ³n actual del agente.\n",
    "- Recompensa: la realimentaciÃ³n que recibe el agente por parte del ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca4c63-e1e6-43d3-ad91-8ee056d7cc78",
   "metadata": {},
   "source": [
    "## Proceso de DecisiÃ³n de Markov\n",
    "\n",
    "Una de las formas clÃ¡sicas de construir un algoritmo de RL es modelar el proceso de decisiÃ³n y los elementos enumerados antes, usando un proceso de decisiÃ³n markoviano (en inglÃ©s Markov Decision Process â€” MDP), que se define como:\n",
    "\n",
    "- $S$: conjunto finito de estados\n",
    "- $A$: conjunto finito de acciones\n",
    "- $P(s' \\mid s, a)$: probabilidad de transiciÃ³n de moverse al estado $s'$ a partir del estado $s$ debido a la acciÃ³n $a$.\n",
    "- $R(s, a)$: recompensa recibida despuÃ©s de tomar la acciÃ³n $a$ en el estado $s$.\n",
    "- $\\gamma\\in[0,1]$: factor de descuento que representa la importancia de recompensas futuras.\n",
    "\n",
    "El objetivo es encontrar una polÃ­tica $\\pi: S \\rightarrow A$ que maximice la recompensa acumulada esperada:\n",
    "$$ V^\\pi(s) = \\mathbb{E}\\left[\\sum_{t=0}^\\infty \\gamma^t R(s_t, a_t) \\mid s_0 = s, a_t = \\pi(s_t) \\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2d2588-7d3a-490c-a6f7-c451f35ddd65",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b324e84-0c87-4fc5-9f0f-ad55314e1ecb",
   "metadata": {},
   "source": [
    "Tabla Q es simplemente un nombre elegante para una tabla de consulta simple donde calculamos las recompensas futuras mÃ¡ximas esperadas por la acciÃ³n en cada estado. BÃ¡sicamente, esta tabla nos guiarÃ¡ hacia la mejor acciÃ³n en cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2781ed-f28e-48f9-b311-aba152595886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example: A Simple Environment with Q-Learning\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define environment\n",
    "states = [0, 1, 2, 3]  # simple environment with 4 states\n",
    "actions = [0, 1]       # 0 = left, 1 = right\n",
    "\n",
    "# Transition and rewards matrix (deterministic)\n",
    "rewards = np.array([\n",
    "    [-1, 0],   # From state 0: left -> -1, right -> 0\n",
    "    [-1, 0],   # From state 1\n",
    "    [-1, 10],  # From state 2: right -> terminal state with reward 10\n",
    "    [0, 0]     # Terminal state\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e7de591-a257-4099-a46e-1ec4c8f3fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "Q = np.zeros((len(states), len(actions)))\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(actions)  # Explore\n",
    "    return np.argmax(Q[state])         # Exploit\n",
    "\n",
    "def update_q_table(state, action, reward, next_state, alpha, gamma):\n",
    "    best_next_action = np.max(Q[next_state])\n",
    "    Q[state, action] += alpha * (reward + gamma * best_next_action - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610cade-96cd-4962-b90f-6cb1ed3b0d22",
   "metadata": {},
   "source": [
    "ActualizaciÃ³n de la tabla Q a partir de las recompensas obtenidas:\n",
    "\n",
    "Definiendo $\\beta_s = \\max_a Q(s,:)$, como la mÃ¡xima recompensa esperada dado un estado $s$, la tabla Q se actualiza a partir de la regla:\n",
    "\n",
    "$$Q(s,a) = Q(s,a) + \\alpha(R(s,a)+\\gamma \\beta_{s'} - Q(s,a))$$\n",
    "\n",
    "donde $s'$ es el estado al que se llega cuando se toma la acciÃ³n $a$ en el estado $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8bda2b-54df-4cf6-a5de-43f1e2675f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[1.87490677 8.09171078]\n",
      " [2.76375056 8.99866509]\n",
      " [5.56084835 9.99973439]\n",
      " [0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_episodes = 100\n",
    "alpha = 0.1     # Learning rate\n",
    "gamma = 0.9     # Discount factor\n",
    "epsilon = 0.2   # Exploration rate\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = 0\n",
    "    while state != 3:\n",
    "        action = choose_action(state, epsilon)\n",
    "        reward = rewards[state, action]\n",
    "        next_state = state + 1 if action == 1 else max(0, state - 1)\n",
    "        update_q_table(state, action, reward, next_state, alpha, gamma)\n",
    "        state = next_state\n",
    "\n",
    "print(\"Trained Q-table:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "344c97fb-e3d5-49d2-9094-63c23bdc336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Policy (0=left, 1=right): [np.int64(1), np.int64(1), np.int64(1), np.int64(0)]\n"
     ]
    }
   ],
   "source": [
    "def extract_policy(Q):\n",
    "    return [np.argmax(Q[state]) for state in range(len(Q))]\n",
    "\n",
    "policy = extract_policy(Q)\n",
    "print(\"Extracted Policy (0=left, 1=right):\", policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae3e0d8-7d3f-45b9-b47e-78a55f47c83d",
   "metadata": {},
   "source": [
    "## Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104060d-26a9-47ec-b506-52d7017f8b86",
   "metadata": {},
   "source": [
    "*Q-learning* con una tabla no es viable para espacios de estado grandes o continuos. *Deep Q-Learning (DQN)* usa una red neuronal para aproximar una *Q-function* que predice la recomensa esperada.\n",
    "\n",
    "Los principales componentes de una DQN son:\n",
    "- Una red neuronal artificial que toma un estado como entrada y retorna los valores Q para cada acciÃ³n.\n",
    "- RepeticiÃ³n de la experiencia: un espacio de memoria para almacenar experiencias pasadas $(s, a, r, s')$.\n",
    "- Red objetivo: una red separada usada para calcular valores Q objetivo actualizada periodicamente.\n",
    "\n",
    "La funciÃ³n de costo usada para entrenar la DQN es:\n",
    "$$ L(\\theta) = \\mathbb{E}_{(s,a,r,s')\\sim D}\\left[\\left(r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a'; \\theta^-) - Q(s,a; \\theta)\\right)^2\\right] $$\n",
    "\n",
    "donde:\n",
    "\n",
    "- $r = R(s,a)$, este valor lo provee el ambiente.\n",
    "- $\\theta$ son los parÃ¡metros de la red Q.\n",
    "- $\\theta^-$ son los parÃ¡metros de la red objetivo.\n",
    "- $D$ es la memoria que almacena las experiencias previas.\n",
    "\n",
    "\n",
    "Durante el entrenamiento, la funciÃ³n de costo se calcula usando:\n",
    "\n",
    "$$ð‘Ÿ + \\gamma \\max_{a'} Q_{\\text{target}}(s',a',\\theta^-)$$\n",
    "\n",
    "en lugar de:\n",
    "\n",
    "$$ð‘Ÿ + \\gamma \\max_{a'} Q(s',a',\\theta^-)$$\n",
    "\n",
    "Luego, cada $n$ pasos o episodios, se copian los pesos de la red de polÃ­ticas a la red objetivo. Se puede pensar en la red objetivo, como un punto de referencia que se mueve mÃ¡s lentamente y permite a la red principal perseguir una seÃ±al de entrenamiento mÃ¡s consistente en lugar de perseguir sus propias y rÃ¡pidamente cambiantes predicciones.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ff2e287-750d-4b12-b812-4f9e99f2cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Python Example of Deep Q-Learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Simple environment\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "\n",
    "# Define neural network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Initialize DQN and optimizer\n",
    "policy_net = DQN()\n",
    "target_net = DQN()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Experience replay buffer\n",
    "memory = deque(maxlen=1000)\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def act(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        return policy_net(state).argmax().item()\n",
    "\n",
    "def replay(batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "    states = torch.FloatTensor(np.array(states))\n",
    "    actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.BoolTensor(dones)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions).squeeze()\n",
    "    next_q_values = target_net(next_states).max(1)[0]\n",
    "    expected_q_values = rewards + (gamma * next_q_values * (~dones))\n",
    "\n",
    "    loss = criterion(q_values, expected_q_values.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def update_target_network():\n",
    "    target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90bfa75b-120c-4168-a28f-716eb1a0d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training loop (hypothetical)\n",
    "for episode in range(10):\n",
    "    state = np.random.rand(state_size)  # Random state for example\n",
    "    for t in range(10):\n",
    "        action = act(state, epsilon=0.1)\n",
    "        next_state = np.random.rand(state_size)\n",
    "\n",
    "        #Real environment must be integrated here\n",
    "        reward = np.random.rand()\n",
    "        \n",
    "        done = t == 9\n",
    "        remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        replay(batch_size=8, gamma=0.95)\n",
    "    \n",
    "    #Update the target network weights periodically by copying weights from the policy network:\n",
    "    if episode % 5 == 0:\n",
    "        update_target_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3c0d2-bb09-46b4-8c40-97e0c68375ca",
   "metadata": {},
   "source": [
    "*Deep Q-Learning* es fundamental para los sistemas de RL modernos y es lo que ha permitido aplicar RL a problemas de alta dimensiÃ³n como jugar juegos de Atari o el control robÃ³tico. El entrenamiento de un modelo de RL, requiere la interacciÃ³n con un ambiente, por lo que existen algunos desarrollos que permiten acceder a problemas y ambientes tÃ­picos. Se recomienda al lector interesado revisar la documentaciÃ³n de [Gymnasium](https://gymnasium.farama.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ef258-ecfc-45df-b556-dfc8fc561f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
